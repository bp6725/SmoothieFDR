{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- SETUP PATHS ---\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from spatial_fdr_evaluation.data.adbench_loader import load_from_ADbench\n",
    "from spatial_fdr_evaluation.methods.kernels import compute_kernel_matrix, estimate_length_scale\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATASET DISCOVERY & SCANNING\n",
    "# ==========================================\n",
    "def get_all_adbench_datasets():\n",
    "    \"\"\"Returns sorted list of all available ADbench classical datasets.\"\"\"\n",
    "    candidate_paths = [\n",
    "        '../third_party/ADbench/datasets/Classical',\n",
    "        '../../third_party/ADbench/datasets/Classical',\n",
    "        './datasets/Classical'\n",
    "    ]\n",
    "    \n",
    "    dataset_dir = None\n",
    "    for p in candidate_paths:\n",
    "        if os.path.exists(p):\n",
    "            dataset_dir = p\n",
    "            break\n",
    "            \n",
    "    if dataset_dir is None:\n",
    "        print(\"Warning: Could not find ADbench dataset directory. Using default list.\")\n",
    "        return ['23_mammography', '37_satellite', '40_vowels', '2_annthyroid', '33_skin', '12_fault']\n",
    "    \n",
    "    files = [f.replace('.npz', '') for f in os.listdir(dataset_dir) if f.endswith('.npz')]\n",
    "    print(f\"Found {len(files)} datasets in {dataset_dir}\")\n",
    "    return sorted(files)\n",
    "\n",
    "def scan_all_datasets(datasets, sigma_factor=0.5, max_samples_for_scan=2000):\n",
    "    \"\"\"\n",
    "    Iterates through ALL datasets to find those with valid spatial blocks.\n",
    "    \"\"\"\n",
    "    valid_results = {}\n",
    "    \n",
    "    print(f\"{'Dataset':<25} | {'N':<6} | {'Gap':<8} | {'k':<3} | {'Status'}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for name in datasets:\n",
    "        try:\n",
    "            # 1. Load Data\n",
    "            data = load_from_ADbench(name)\n",
    "            X_full = data['X_train']\n",
    "            \n",
    "            # Subsample for speed during scan\n",
    "            if len(X_full) > max_samples_for_scan:\n",
    "                idx = np.random.choice(len(X_full), max_samples_for_scan, replace=False)\n",
    "                X = StandardScaler().fit_transform(X_full[idx])\n",
    "            else:\n",
    "                X = StandardScaler().fit_transform(X_full)\n",
    "                \n",
    "            # 2. Compute Kernel (Sharper Sigma)\n",
    "            sigma = estimate_length_scale(X, method='median') * sigma_factor\n",
    "            K = compute_kernel_matrix(X, kernel_type='rbf', length_scale=sigma)\n",
    "            \n",
    "            # 3. Spectral Analysis\n",
    "            D = np.array(K.sum(axis=1)).flatten() + 1e-10\n",
    "            A_norm = (np.diag(1/np.sqrt(D)) @ K @ np.diag(1/np.sqrt(D)))\n",
    "            \n",
    "            try:\n",
    "                evals, evecs = eigsh(A_norm, k=10, which='LA')\n",
    "            except:\n",
    "                evals, evecs = linalg.eigh(A_norm)\n",
    "                evals, evecs = evals[-10:], evecs[:, -10:]\n",
    "            \n",
    "            idx = np.argsort(evals)[::-1]\n",
    "            evals = evals[idx]\n",
    "            evecs = evecs[:, idx]\n",
    "            \n",
    "            # 4. Gap Analysis\n",
    "            gaps = np.diff(1 - evals) \n",
    "            valid_gaps = gaps[1:] # Ignore k=1\n",
    "            \n",
    "            if len(valid_gaps) == 0:\n",
    "                print(f\"{name:<25} | {len(X):<6} | {'N/A':<8} | -   | REJECT (Flat)\")\n",
    "                continue\n",
    "\n",
    "            local_best_idx = np.argmax(valid_gaps)\n",
    "            optimal_k = (local_best_idx + 1) + 1 \n",
    "            max_gap = valid_gaps[local_best_idx]\n",
    "            \n",
    "            if max_gap < 0.02: \n",
    "                 print(f\"{name:<25} | {len(X):<6} | {max_gap:.4f}   | {optimal_k:<3} | REJECT (Gap too small)\")\n",
    "                 continue\n",
    "\n",
    "            # 5. Check Block Sizes\n",
    "            kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(evecs[:, :optimal_k])\n",
    "            \n",
    "            counts = np.bincount(labels)\n",
    "            valid_blocks = np.sum(counts >= 50)\n",
    "            \n",
    "            if valid_blocks < 2:\n",
    "                print(f\"{name:<25} | {len(X):<6} | {max_gap:.4f}   | {optimal_k:<3} | REJECT (Only 1 valid block)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"{name:<25} | {len(X):<6} | {max_gap:.4f}   | {optimal_k:<3} | OK\")\n",
    "            \n",
    "            valid_results[name] = {\n",
    "                'gap': max_gap,\n",
    "                'k': optimal_k,\n",
    "                'sigma_factor': sigma_factor\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass \n",
    "            \n",
    "    print(\"-\" * 75)\n",
    "    print(f\"Found {len(valid_results)} suitable datasets.\")\n",
    "    return valid_results\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. STRATIFIED SAMPLING\n",
    "# ==========================================\n",
    "def run_stratified_experiment(dataset_name, config, n_h1=50, n_h0_compact=150, n_h0_close=30, n_h0_far=270):\n",
    "    \"\"\"\n",
    "    Loads dataset and performs 4-group stratified sampling.\n",
    "    \"\"\"\n",
    "    # 1. Reload Full Data\n",
    "    data = load_from_ADbench(dataset_name)\n",
    "    X = StandardScaler().fit_transform(data['X_train'])\n",
    "    \n",
    "    # 2. Recompute Kernel\n",
    "    sigma = estimate_length_scale(X, method='median') * config['sigma_factor']\n",
    "    K = compute_kernel_matrix(X, kernel_type='rbf', length_scale=sigma)\n",
    "    \n",
    "    D = np.array(K.sum(axis=1)).flatten() + 1e-10\n",
    "    A_norm = (np.diag(1/np.sqrt(D)) @ K @ np.diag(1/np.sqrt(D)))\n",
    "    \n",
    "    try:\n",
    "        evals, evecs = eigsh(A_norm, k=10, which='LA')\n",
    "    except:\n",
    "        evals, evecs = linalg.eigh(A_norm)\n",
    "        evals, evecs = evals[-10:], evecs[:, -10:]\n",
    "        \n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evecs = evecs[:, idx]\n",
    "    \n",
    "    optimal_k = config['k']\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    labels_full = kmeans.fit_predict(evecs[:, :optimal_k])\n",
    "    \n",
    "    # --- SAMPLING LOGIC ---\n",
    "    valid_clusters = [c for c in range(optimal_k) if np.sum(labels_full == c) >= n_h1]\n",
    "    if not valid_clusters: return None\n",
    "    \n",
    "    signal_cid = np.random.choice(valid_clusters)\n",
    "    pool_h1 = np.where(labels_full == signal_cid)[0]\n",
    "    \n",
    "    # Partition Noise\n",
    "    pool_noise_all = np.where(labels_full != signal_cid)[0]\n",
    "    sim_matrix_h1 = K[np.ix_(pool_h1, pool_h1)]\n",
    "    medoid_idx = pool_h1[np.argmax(sim_matrix_h1.sum(axis=1))]\n",
    "    sim_to_signal = K[pool_noise_all, medoid_idx]\n",
    "    \n",
    "    thresh_close = np.percentile(sim_to_signal, 80)\n",
    "    thresh_far = np.percentile(sim_to_signal, 50)\n",
    "    \n",
    "    pool_close = pool_noise_all[sim_to_signal >= thresh_close]\n",
    "    pool_far = pool_noise_all[sim_to_signal <= thresh_far]\n",
    "    \n",
    "    # Compact H0 selection\n",
    "    other_clusters = [c for c in range(optimal_k) if c != signal_cid]\n",
    "    if len(other_clusters) >= 1 and optimal_k > 2:\n",
    "        compact_cid = np.random.choice(other_clusters)\n",
    "        pool_compact = np.where(labels_full == compact_cid)[0]\n",
    "    else:\n",
    "        mask_middle = (sim_to_signal > thresh_far) & (sim_to_signal < thresh_close)\n",
    "        pool_compact = pool_noise_all[mask_middle]\n",
    "\n",
    "    # Quota Adjustment\n",
    "    if len(pool_compact) < n_h0_compact:\n",
    "        missing = n_h0_compact - len(pool_compact)\n",
    "        n_h0_compact = len(pool_compact)\n",
    "        n_h0_far += missing\n",
    "        \n",
    "    if len(pool_close) < n_h0_close: n_h0_close = len(pool_close)\n",
    "    if len(pool_far) < n_h0_far:     n_h0_far = len(pool_far)\n",
    "    \n",
    "    idx_h1 = np.random.choice(pool_h1, n_h1, replace=False)\n",
    "    idx_compact = np.random.choice(pool_compact, n_h0_compact, replace=False) if n_h0_compact > 0 else []\n",
    "    idx_close = np.random.choice(pool_close, n_h0_close, replace=False)\n",
    "    idx_far = np.random.choice(pool_far, n_h0_far, replace=False)\n",
    "    \n",
    "    final_indices = np.concatenate([idx_h1, idx_compact, idx_close, idx_far]).astype(int)\n",
    "    np.random.shuffle(final_indices)\n",
    "    \n",
    "    true_labels = np.ones(len(final_indices), dtype=int)\n",
    "    true_labels[np.isin(final_indices, idx_h1)] = 0\n",
    "    \n",
    "    group_ids = np.zeros(len(final_indices), dtype=int)\n",
    "    group_ids[np.isin(final_indices, idx_compact)] = 1\n",
    "    group_ids[np.isin(final_indices, idx_close)] = 2\n",
    "    group_ids[np.isin(final_indices, idx_far)] = 3\n",
    "    \n",
    "    return {\n",
    "        'X': X[final_indices],\n",
    "        'K': K[np.ix_(final_indices, final_indices)],\n",
    "        'true_labels': true_labels,\n",
    "        'group_ids': group_ids,\n",
    "        'indices': final_indices\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. COMPREHENSIVE VISUALIZATION\n",
    "# ==========================================\n",
    "def verify_dataset_comprehensive(experiment):\n",
    "    K = experiment['K']\n",
    "    g_ids = experiment['group_ids']\n",
    "    labels = [\"H1 Signal\", \"H0 Compact\", \"H0 Close\", \"H0 Far\"]\n",
    "    \n",
    "    # A. Matrix Clustermap (Sorted by Group)\n",
    "    sort_idx = np.argsort(g_ids)\n",
    "    K_sorted = K[np.ix_(sort_idx, sort_idx)]\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(K_sorted, cmap=\"viridis\", cbar=False, xticklabels=False, yticklabels=False)\n",
    "    plt.title(\"Kernel Matrix (Sorted)\\nTop=H1, Blue=Compact, Orange=Close, Gray=Far\")\n",
    "    plt.show()\n",
    "    \n",
    "    # B. Similarity to Signal (Boxplot)\n",
    "    h1_indices = np.where(g_ids == 0)[0]\n",
    "    sims = []\n",
    "    names = []\n",
    "    \n",
    "    sims.append(K[np.ix_(h1_indices, h1_indices)].mean(axis=1))\n",
    "    names.append(\"H1 Self\")\n",
    "    \n",
    "    for gid in [1, 2, 3]:\n",
    "        idx = np.where(g_ids == gid)[0]\n",
    "        if len(idx) > 0:\n",
    "            sims.append(K[np.ix_(idx, h1_indices)].mean(axis=1))\n",
    "            names.append(f\"{labels[gid]}\")\n",
    "            \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.boxplot(sims, labels=names)\n",
    "    plt.title(\"Similarity to Signal Block (H1)\")\n",
    "    plt.ylabel(\"Kernel Value\")\n",
    "    plt.show()\n",
    "\n",
    "    # C. Separation Stats\n",
    "    k_h1_h1 = K[np.ix_(h1_indices, h1_indices)]\n",
    "    sim_h1_h1 = k_h1_h1[~np.eye(len(h1_indices), dtype=bool)].mean()\n",
    "    \n",
    "    # H1 vs All Noise\n",
    "    noise_idx = np.where(g_ids != 0)[0]\n",
    "    sim_h1_noise = K[np.ix_(h1_indices, noise_idx)].mean()\n",
    "    \n",
    "    print(f\"  > Signal Coherence (H1-H1): {sim_h1_h1:.4f}\")\n",
    "    print(f\"  > Noise Separation (H1-All): {sim_h1_noise:.4f}\")\n",
    "    print(f\"  > GAP: {sim_h1_h1 - sim_h1_noise:.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION (LOOP ALL)\n",
    "# ==========================================\n",
    "\n",
    "# A. Discovery\n",
    "all_datasets = get_all_adbench_datasets()\n",
    "\n",
    "# B. Scan\n",
    "valid_datasets = scan_all_datasets(all_datasets, sigma_factor=0.5)\n",
    "\n",
    "# C. Process ALL Candidates\n",
    "if valid_datasets:\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"STARTING ANALYSIS OF {len(valid_datasets)} CANDIDATES\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Sort by Gap score for better presentation\n",
    "    sorted_candidates = sorted(valid_datasets.items(), key=lambda x: x[1]['gap'], reverse=True)\n",
    "    \n",
    "    for name, config in sorted_candidates:\n",
    "        print(f\"\\n>>> DATASET: {name} (Gap: {config['gap']:.4f}, k={config['k']})\")\n",
    "        \n",
    "        try:\n",
    "            experiment = run_stratified_experiment(\n",
    "                name, \n",
    "                config,\n",
    "                n_h1=50, \n",
    "                n_h0_compact=150, \n",
    "                n_h0_close=20, \n",
    "                n_h0_far=280\n",
    "            )\n",
    "            \n",
    "            if experiment:\n",
    "                verify_dataset_comprehensive(experiment)\n",
    "            else:\n",
    "                print(\"  [Failed to sample valid groups]\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  [Error processing {name}: {str(e)}]\")\n",
    "else:\n",
    "    print(\"No suitable datasets found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MaxEnt)",
   "language": "python",
   "name": "maxentropyrf_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
