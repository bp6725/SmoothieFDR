{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from spatial_fdr_evaluation.data.adbench_loader import load_from_ADbench\n",
    "from spatial_fdr_evaluation.methods.kernels import compute_kernel_matrix, estimate_length_scale\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_full_dataset(dataset_name, sigma_factor=1.0):\n",
    "    \"\"\"\n",
    "    Loads full dataset, computes kernel, and checks spectral gap.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing {dataset_name} ---\")\n",
    "    \n",
    "    # 1. Load & Scale Full Data\n",
    "    data = load_from_ADbench(dataset_name)\n",
    "    X_full = data['X_train']\n",
    "    scaler = StandardScaler()\n",
    "    X_full = scaler.fit_transform(X_full)\n",
    "    \n",
    "    # 2. Compute Global Kernel\n",
    "    base_sigma = estimate_length_scale(X_full, method='median')\n",
    "    sigma = base_sigma * sigma_factor\n",
    "    print(f\"  N={len(X_full)}, D={X_full.shape[1]}\")\n",
    "    print(f\"  Sigma: {sigma:.4f}\")\n",
    "    \n",
    "    K_full = compute_kernel_matrix(X_full, kernel_type='rbf', length_scale=sigma)\n",
    "    \n",
    "    # 3. Spectral Eigengap (Check for Blocks)\n",
    "    # Normalized Affinity: D^-1/2 K D^-1/2\n",
    "    D = np.array(K_full.sum(axis=1)).flatten()\n",
    "    D[D < 1e-10] = 1e-10\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(D))\n",
    "    A_norm = D_inv_sqrt @ K_full @ D_inv_sqrt\n",
    "    \n",
    "    # Get largest eigenvalues (equivalent to smallest Laplacian eigenvalues)\n",
    "    try:\n",
    "        evals, evecs = eigsh(A_norm, k=15, which='LA')\n",
    "    except:\n",
    "        evals, evecs = linalg.eigh(A_norm)\n",
    "        evals, evecs = evals[-15:], evecs[:, -15:]\n",
    "        \n",
    "    # Sort descending\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "    \n",
    "    # Gap analysis (on Laplacian scale: 1 - lambda)\n",
    "    gaps = np.diff(1 - evals)\n",
    "    optimal_k = np.argmax(gaps) + 1\n",
    "    max_gap = gaps[optimal_k-1]\n",
    "    \n",
    "    print(f\"  Max Eigengap: {max_gap:.4f} at k={optimal_k}\")\n",
    "    \n",
    "    return {\n",
    "        'X': X_full,\n",
    "        'K': K_full,\n",
    "        'evecs': evecs,\n",
    "        'optimal_k': optimal_k,\n",
    "        'sigma': sigma,\n",
    "        'max_gap': max_gap\n",
    "    }\n",
    "\n",
    "# Run\n",
    "global_res = analyze_full_dataset('2_annthyroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_experiment(global_res, n_total=500, h1_ratio=0.1):\n",
    "    \"\"\"\n",
    "    1. Defines Signal Region on FULL dataset.\n",
    "    2. Subsamples to get exact H1/H0 ratio.\n",
    "    \"\"\"\n",
    "    X = global_res['X']\n",
    "    optimal_k = global_res['optimal_k']\n",
    "    evecs = global_res['evecs']\n",
    "    \n",
    "    # 1. Define Signal Block on Full Data\n",
    "    # Use spectral embedding to find the 'tightest' block\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    labels_full = kmeans.fit_predict(evecs[:, :optimal_k])\n",
    "    \n",
    "    # Pick a random cluster to be the \"Signal Source\"\n",
    "    signal_cluster_id = np.random.randint(optimal_k)\n",
    "    h1_candidates = np.where(labels_full == signal_cluster_id)[0]\n",
    "    h0_candidates = np.where(labels_full != signal_cluster_id)[0]\n",
    "    \n",
    "    # 2. Subsample\n",
    "    n_h1 = int(n_total * h1_ratio)\n",
    "    n_h0 = n_total - n_h1\n",
    "    \n",
    "    # Randomly draw required counts\n",
    "    if len(h1_candidates) < n_h1:\n",
    "        print(f\"Warning: Signal cluster too small ({len(h1_candidates)}), taking all.\")\n",
    "        n_h1 = len(h1_candidates)\n",
    "        \n",
    "    idx_h1 = np.random.choice(h1_candidates, n_h1, replace=False)\n",
    "    idx_h0 = np.random.choice(h0_candidates, n_h0, replace=False)\n",
    "    \n",
    "    final_indices = np.concatenate([idx_h1, idx_h0])\n",
    "    # Shuffle so H1s aren't all at the start\n",
    "    np.random.shuffle(final_indices)\n",
    "    \n",
    "    # 3. Create Final Experiment Set\n",
    "    X_sub = X[final_indices]\n",
    "    \n",
    "    # Recompute K for the subset (Standard practice: Kernel depends on X_sub)\n",
    "    # Alternatively, slice K_full. Slicing is faster and consistent.\n",
    "    K_sub = global_res['K'][np.ix_(final_indices, final_indices)]\n",
    "    \n",
    "    # Create Labels (0=H1, 1=H0)\n",
    "    # We stick to: 0=H1 (Alternative), 1=H0 (Null) as per your previous code\n",
    "    true_labels = np.ones(n_total, dtype=int)\n",
    "    # We need to find where the h1 indices ended up after shuffle\n",
    "    # Easier way: create label array aligned with final_indices\n",
    "    is_h1 = np.isin(final_indices, idx_h1)\n",
    "    true_labels[is_h1] = 0\n",
    "    \n",
    "    return {\n",
    "        'X': X_sub,\n",
    "        'K': K_sub,\n",
    "        'true_labels': true_labels,\n",
    "        'indices': final_indices\n",
    "    }\n",
    "\n",
    "experiment = subsample_experiment(global_res, n_total=500, h1_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kernel_separation(experiment):\n",
    "    \"\"\"\n",
    "    Validates if H1 points are actually closer to each other than to H0s.\n",
    "    \"\"\"\n",
    "    K = experiment['K']\n",
    "    labels = experiment['true_labels']\n",
    "    \n",
    "    h1_idx = np.where(labels == 0)[0]\n",
    "    h0_idx = np.where(labels == 1)[0]\n",
    "    \n",
    "    # Extract Pairwise Similarities\n",
    "    # H1-H1 (exclude diagonal self-similarity)\n",
    "    k_h1_h1 = K[np.ix_(h1_idx, h1_idx)]\n",
    "    sim_h1_h1 = k_h1_h1[~np.eye(k_h1_h1.shape[0], dtype=bool)]\n",
    "    \n",
    "    # H0-H0\n",
    "    k_h0_h0 = K[np.ix_(h0_idx, h0_idx)]\n",
    "    sim_h0_h0 = k_h0_h0[~np.eye(k_h0_h0.shape[0], dtype=bool)]\n",
    "    \n",
    "    # H1-H0 (Across)\n",
    "    k_h1_h0 = K[np.ix_(h1_idx, h0_idx)]\n",
    "    sim_h1_h0 = k_h1_h0.flatten()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data_to_plot = [sim_h1_h1, sim_h0_h0, sim_h1_h0]\n",
    "    plt.boxplot(data_to_plot, labels=['H1-H1 (Within Signal)', 'H0-H0 (Within Noise)', 'H1-H0 (Across)'])\n",
    "    plt.title(\"Kernel Similarity Distribution\")\n",
    "    plt.ylabel(\"Kernel Value $K(x,y)$\")\n",
    "    \n",
    "    # Quantify\n",
    "    mean_h1 = np.mean(sim_h1_h1)\n",
    "    mean_across = np.mean(sim_h1_h0)\n",
    "    print(f\"Mean H1-H1 Similarity: {mean_h1:.4f}\")\n",
    "    print(f\"Mean Across Similarity: {mean_across:.4f}\")\n",
    "    \n",
    "    if mean_h1 > mean_across + 0.1: # Threshold is arbitrary, but gap should be visible\n",
    "        print(\"PASS: H1 points are significantly closer to each other.\")\n",
    "    else:\n",
    "        print(\"FAIL/WARNING: H1 points are not distinct from background.\")\n",
    "\n",
    "test_kernel_separation(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_visualize(experiment):\n",
    "    labels = experiment['true_labels']\n",
    "    \n",
    "    # Generate P-values\n",
    "    p_values = np.zeros(len(labels))\n",
    "    p_values[labels==1] = np.random.uniform(0, 1, size=(labels==1).sum()) # Nulls\n",
    "    p_values[labels==0] = np.random.beta(0.1, 1, size=(labels==0).sum())  # Alts\n",
    "    \n",
    "    # Oracle View (Isolation Plot)\n",
    "    # Calculate Isolation for H1s\n",
    "    K = experiment['K']\n",
    "    \n",
    "    # Isolation = 1 - Mean sim to top 5 other H1s\n",
    "    # (We compute this for ALL points to see the separation)\n",
    "    K_nodiag = K.copy()\n",
    "    np.fill_diagonal(K_nodiag, 0)\n",
    "    \n",
    "    # Sort neighbors\n",
    "    nearest_sims = np.sort(K_nodiag, axis=1)[:, ::-1]\n",
    "    top_k_sim = np.mean(nearest_sims[:, :10], axis=1)\n",
    "    isolation = 1 - top_k_sim\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(isolation[labels==1], np.log10(p_values[labels==1]+1e-10), \n",
    "                c='blue', alpha=0.3, label='H0 (Null)')\n",
    "    plt.scatter(isolation[labels==0], np.log10(p_values[labels==0]+1e-10), \n",
    "                c='red', alpha=0.8, label='H1 (Signal)')\n",
    "    \n",
    "    plt.xlabel(\"Kernel Isolation (1 - Sim)\")\n",
    "    plt.ylabel(\"log10(p-value)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Evaluation Set: Difficulty Check\")\n",
    "    plt.show()\n",
    "\n",
    "generate_and_visualize(experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
